# Ollama with Gemma 2B Configuration
# ==================================

# Basic LLM Configuration
llm.enabled=true
llm.model.type=ollama

# Disable other LLM options
llm.local.enabled=false
llm.gemini.enabled=false

# Ollama Configuration
# ===================

# Enable Ollama
llm.ollama.enabled=true

# Ollama server URL (default local installation)
llm.ollama.url=http://localhost:11434

# Ollama model to use
# Popular options:
# - gemma2:2b (lightweight, fast)
# - gemma2:9b (more capable, slower)
# - llama3.1:8b (good balance)
# - mistral:7b (alternative option)
llm.ollama.model=gemma2:2b

# Rate Limiting Configuration
# ===========================

# Enable automatic retry when hitting rate limits
# (Less needed for local Ollama, but good for consistency)
llm.rate.limit.retry.enabled=true

# Maximum number of retries for rate limiting
# For local Ollama, 1-2 retries are usually sufficient
llm.rate.limit.max.retries=2

# Smart Input Fetching Configuration
# ==================================
smart.input.fetch.enabled=true
smart.input.fetch.percentage=0.9
smart.input.fetch.llm.discovery.enabled=true
smart.input.fetch.max.candidates=5

# LLM Communication Logging Configuration
# =======================================
llm.communication.logging.enabled=true
llm.communication.logging.dir=logs/llm-communications
llm.communication.logging.file.prefix=llm-communication
llm.communication.logging.include.response.time=true
llm.communication.logging.include.content=true
llm.communication.logging.include.metadata=true

# System Resource Monitoring Configuration (OLLAMA MODEL)
# =======================================================
llm.resource.monitoring.enabled=true
llm.resource.monitoring.dir=logs/system-resources
llm.resource.monitoring.file.prefix=resource-monitor
llm.resource.monitoring.interval.ms=1000
llm.resource.monitoring.include.detailed.memory=true
llm.resource.monitoring.include.cpu=true
llm.resource.monitoring.include.system.info=true

# Expected Behavior with Ollama + Gemma 2B:
# =========================================
#
# When using Ollama with Gemma 2B, you'll see:
#
# âœ… [LLMService] Using Ollama API for generation
# ðŸ§  Calling LLM for DIRECT VALUE EXTRACTION (not JSONPath) for parameter 'endStation'
# âœ… LLM extracted ACTUAL VALUE 'Shanghai' for parameter 'endStation' (not JSONPath)
# âœ… Smart Fetch Success: endStation = 'Shanghai'
#
# Benefits of Ollama + Gemma 2B:
# - Fast local inference (no API calls)
# - No rate limiting issues
# - Privacy (data stays local)
# - Consistent availability
# - Good quality for parameter generation

# Ollama Setup Instructions:
# =========================
#
# 1. Install Ollama:
#    - Download from: https://ollama.ai/
#    - Or use: curl -fsSL https://ollama.ai/install.sh | sh
#
# 2. Pull Gemma 2B model:
#    ollama pull gemma2:2b
#
# 3. Verify installation:
#    ollama list
#    ollama run gemma2:2b "Hello, how are you?"
#
# 4. Start Ollama server (if not auto-started):
#    ollama serve
#
# 5. Test API endpoint:
#    curl http://localhost:11434/api/tags

# Alternative Models:
# ==================
#
# If you want to try different models, change llm.ollama.model to:
#
# Lightweight options:
# llm.ollama.model=gemma2:2b      # 1.6GB, very fast
# llm.ollama.model=phi3:mini      # 2.3GB, Microsoft model
#
# Balanced options:
# llm.ollama.model=gemma2:9b      # 5.4GB, more capable
# llm.ollama.model=llama3.1:8b    # 4.7GB, Meta model
#
# Larger options (if you have GPU/RAM):
# llm.ollama.model=llama3.1:70b   # 40GB, very capable
# llm.ollama.model=gemma2:27b     # 16GB, Google's largest

# Troubleshooting:
# ===============
#
# If you see connection errors:
# - Check if Ollama is running: ollama list
# - Verify URL: curl http://localhost:11434/api/tags
# - Check firewall settings
#
# If model responses are poor:
# - Try a larger model (gemma2:9b instead of gemma2:2b)
# - Adjust temperature in code (lower = more focused)
#
# If generation is slow:
# - Use smaller model (gemma2:2b)
# - Enable GPU acceleration if available
# - Reduce max_tokens in prompts

# Performance Expectations:
# ========================
#
# Gemma 2B on typical hardware:
# - CPU only: 2-5 tokens/second
# - With GPU: 20-50 tokens/second
# - Memory usage: ~2GB RAM
#
# For smart fetch (short responses):
# - Response time: 1-3 seconds per parameter
# - Total test generation: 30-60 seconds for 10 parameters
# - Much faster than API calls with rate limiting!

# Integration with Smart Fetch:
# =============================
#
# Smart fetch will use Ollama for:
# 1. Endpoint discovery (selecting best API for parameter)
# 2. Direct value extraction (getting actual values from API responses)
# 3. Value generation (creating meaningful fallback values)
#
# All three use cases benefit from local, fast inference!
