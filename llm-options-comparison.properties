# LLM Options Comparison - Choose Your Preferred Option
# =====================================================

# You now have THREE LLM options to choose from:
# 1. Gemini API (Google Cloud)
# 2. Local LLM (GPT4All)  
# 3. Ollama + Gemma 2B (Local modern models) ‚Üê RECOMMENDED

# ===========================================
# OPTION 1: GEMINI API (Google Cloud)
# ===========================================
# Pros: Highest quality, latest models
# Cons: Rate limited, requires API key, costs money
#
# llm.enabled=true
# llm.model.type=gemini
# llm.gemini.enabled=true
# llm.gemini.api.key=YOUR_API_KEY_HERE
# llm.gemini.model=gemini-2.0-flash-exp
# llm.gemini.api.url=https://generativelanguage.googleapis.com/v1beta/models
# llm.rate.limit.retry.enabled=true
# llm.rate.limit.max.retries=3
# llm.local.enabled=false
# llm.ollama.enabled=false

# ===========================================
# OPTION 2: LOCAL LLM (GPT4All)
# ===========================================
# Pros: Private, no API costs, offline
# Cons: Slower, basic models, requires setup
#
# llm.enabled=true
# llm.model.type=local
# llm.local.enabled=true
# llm.local.url=http://localhost:4891
# llm.local.model=Meta-Llama-3-8B-Instruct.Q4_0.gguf
# llm.gemini.enabled=false
# llm.ollama.enabled=false

# ===========================================
# OPTION 3: OLLAMA + GEMMA 2B (RECOMMENDED)
# ===========================================
# Pros: Fast, modern models, private, easy setup
# Cons: Requires Ollama installation
#
llm.enabled=true
llm.model.type=ollama
llm.ollama.enabled=true
llm.ollama.url=http://localhost:11434
llm.ollama.model=gemma2:2b
llm.rate.limit.retry.enabled=true
llm.rate.limit.max.retries=2
llm.local.enabled=false
llm.gemini.enabled=false

# Smart Input Fetching Configuration
# ==================================
smart.input.fetch.enabled=true
smart.input.fetch.percentage=0.9
smart.input.fetch.llm.discovery.enabled=true
smart.input.fetch.max.candidates=5

# LLM Communication Logging Configuration
# =======================================
llm.communication.logging.enabled=true
llm.communication.logging.dir=logs/llm-communications
llm.communication.logging.file.prefix=llm-communication
llm.communication.logging.include.response.time=true
llm.communication.logging.include.content=true
llm.communication.logging.include.metadata=true

# System Resource Monitoring Configuration (LOCAL & OLLAMA ONLY)
# ==============================================================
llm.resource.monitoring.enabled=true
llm.resource.monitoring.dir=logs/system-resources
llm.resource.monitoring.file.prefix=resource-monitor
llm.resource.monitoring.interval.ms=1000
llm.resource.monitoring.include.detailed.memory=true
llm.resource.monitoring.include.cpu=true
llm.resource.monitoring.include.system.info=true

# Performance Comparison:
# ======================
#
# Test Generation Time (10 parameters):
# - Gemini API: 5-20 minutes (due to rate limits)
# - Local GPT4All: 2-5 minutes (slower inference)
# - Ollama Gemma2:2b: 30-60 seconds ‚úÖ FASTEST
#
# Quality Comparison:
# - Gemini API: Excellent (9/10)
# - Local GPT4All: Good (7/10)
# - Ollama Gemma2:2b: Very Good (8/10) ‚úÖ BEST BALANCE
#
# Setup Difficulty:
# - Gemini API: Easy (just API key) but costs money
# - Local GPT4All: Medium (model download, server setup)
# - Ollama Gemma2:2b: Easy (ollama pull gemma2:2b) ‚úÖ EASIEST
#
# Privacy:
# - Gemini API: Data sent to Google
# - Local GPT4All: Completely private ‚úÖ
# - Ollama Gemma2:2b: Completely private ‚úÖ

# Quick Setup for Ollama (Recommended):
# =====================================
#
# 1. Install Ollama:
#    curl -fsSL https://ollama.ai/install.sh | sh
#
# 2. Pull model:
#    ollama pull gemma2:2b
#
# 3. Verify:
#    ollama list
#    curl http://localhost:11434/api/tags
#
# 4. Use this config file!

# Alternative Ollama Models:
# =========================
#
# For even better quality (if you have more RAM/GPU):
# llm.ollama.model=gemma2:9b      # 5.4GB, more capable
# llm.ollama.model=llama3.1:8b    # 4.7GB, Meta model
#
# For maximum speed (if quality is less important):
# llm.ollama.model=phi3:mini      # 2.3GB, Microsoft model
#
# For maximum quality (if you have powerful hardware):
# llm.ollama.model=llama3.1:70b   # 40GB, very capable
# llm.ollama.model=gemma2:27b     # 16GB, Google's largest

# Troubleshooting:
# ===============
#
# If Ollama connection fails:
# - Check if Ollama is running: ollama list
# - Verify API: curl http://localhost:11434/api/tags
# - Check model is pulled: ollama list
#
# If generation is slow:
# - Use smaller model: gemma2:2b instead of gemma2:9b
# - Enable GPU if available
# - Check system resources
#
# If quality is poor:
# - Try larger model: gemma2:9b instead of gemma2:2b
# - Adjust temperature in code (lower = more focused)
# - Check prompt engineering

# Expected Log Output with Ollama:
# ================================
#
# LLMConfig initialized: enabled=true, modelType=OLLAMA, ollamaEnabled=true
# OllamaApiClient initialized with model: gemma2:2b, baseUrl: http://localhost:11434
# ‚úÖ [LLMService] Using Ollama API for generation
# üß† LLM endpoint selection for parameter 'endStation'...
# ‚úÖ LLM selected GET endpoint '/api/v1/stationservice/stations' for parameter 'endStation'
# üîÑ Starting DIRECT VALUE EXTRACTION for parameter 'endStation'
# ‚úÖ LLM extracted ACTUAL VALUE 'Shanghai' for parameter 'endStation' (not JSONPath)
# ‚úÖ Smart Fetch Success: endStation = 'Shanghai'
#
# Notice: No rate limiting messages, fast responses!
