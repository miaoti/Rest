# System Resource Monitoring Configuration for Local LLM Models
# =============================================================

# Basic LLM Configuration (can be combined with existing config)
llm.enabled=true
# Choose your LLM type: local, ollama, or gemini
# Resource monitoring ONLY works with LOCAL and OLLAMA models
llm.model.type=local

# LLM Communication Logging Configuration
# =======================================
llm.communication.logging.enabled=true
llm.communication.logging.dir=logs/llm-communications
llm.communication.logging.file.prefix=llm-communication
llm.communication.logging.include.response.time=true
llm.communication.logging.include.content=true
llm.communication.logging.include.metadata=true
llm.communication.logging.level=INFO
llm.communication.logging.max.content.length=10000

# System Resource Monitoring Configuration
# ========================================

# Enable/disable resource monitoring
# Note: Only monitors LOCAL and OLLAMA models, not online APIs like Gemini
llm.resource.monitoring.enabled=true

# Directory for resource monitoring logs
llm.resource.monitoring.dir=logs/system-resources

# File prefix for resource monitoring logs
llm.resource.monitoring.file.prefix=resource-monitor

# Monitoring interval in milliseconds (how often to sample resources)
# Recommended values:
# - 500ms for detailed monitoring
# - 1000ms for balanced monitoring (default)
# - 2000ms for lighter monitoring
llm.resource.monitoring.interval.ms=1000

# Include detailed memory information (heap, non-heap)
llm.resource.monitoring.include.detailed.memory=true

# Include CPU usage information
llm.resource.monitoring.include.cpu=true

# Include system information at the start of monitoring
llm.resource.monitoring.include.system.info=true

# Sample Configurations for Different Use Cases
# =============================================

# ==========================================
# CONFIGURATION 1: LOCAL MODEL (GPT4All)
# ==========================================
# Use this for GPT4All local model monitoring

# Uncomment for local model:
# llm.model.type=local
# llm.local.enabled=true
# llm.local.url=http://localhost:4891
# llm.local.model=Meta-Llama-3-8B-Instruct.Q4_0.gguf
# llm.gemini.enabled=false
# llm.ollama.enabled=false

# ==========================================
# CONFIGURATION 2: OLLAMA MODEL
# ==========================================
# Use this for Ollama model monitoring

# Uncomment for Ollama model:
# llm.model.type=ollama
# llm.ollama.enabled=true
# llm.ollama.url=http://localhost:11434
# llm.ollama.model=gemma2:2b
# llm.local.enabled=false
# llm.gemini.enabled=false

# ==========================================
# CONFIGURATION 3: GEMINI API (NO MONITORING)
# ==========================================
# Resource monitoring will be DISABLED for Gemini API calls

# Uncomment for Gemini (no resource monitoring):
# llm.model.type=gemini
# llm.gemini.enabled=true
# llm.gemini.api.key=YOUR_API_KEY_HERE
# llm.gemini.model=gemini-2.0-flash-exp
# llm.gemini.api.url=https://generativelanguage.googleapis.com/v1beta/models
# llm.local.enabled=false
# llm.ollama.enabled=false

# Expected Behavior
# =================
#
# 1. For LOCAL and OLLAMA models:
#    - Resource monitoring logs will be created in logs/system-resources/
#    - CPU and memory usage will be tracked every 1 second during LLM calls
#    - Communication logs will show "Resource Monitoring: ✅ ENABLED"
#
# 2. For GEMINI API calls:
#    - No resource monitoring (it's an online API)
#    - Communication logs will show "Resource Monitoring: ⏭️ SKIPPED (Online Model)"
#
# 3. If resource monitoring is disabled:
#    - Communication logs will show "Resource Monitoring: ❌ DISABLED"

# Performance Tips
# ================
#
# 1. Monitoring Interval:
#    - Lower intervals (500ms) = more detailed data, higher overhead
#    - Higher intervals (2000ms) = less detailed data, lower overhead
#
# 2. For production use:
#    - Consider setting llm.resource.monitoring.interval.ms=2000
#    - Set llm.resource.monitoring.include.detailed.memory=false for less verbose logs
#
# 3. For debugging/analysis:
#    - Use llm.resource.monitoring.interval.ms=500 for fine-grained monitoring
#    - Keep all monitoring features enabled

# Advanced Configuration
# ======================

# To disable resource monitoring entirely:
# llm.resource.monitoring.enabled=false

# To monitor only memory (not CPU):
# llm.resource.monitoring.include.cpu=false

# For minimal monitoring (memory only, every 5 seconds):
# llm.resource.monitoring.interval.ms=5000
# llm.resource.monitoring.include.cpu=false
# llm.resource.monitoring.include.detailed.memory=false
# llm.resource.monitoring.include.system.info=false
